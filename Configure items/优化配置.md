```sql
开启优化: 
--分区
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=10000;
set hive.exec.max.dynamic.partitions=100000;
set hive.exec.max.created.files=150000;
--hive压缩
set hive.exec.compress.intermediate=true;
set hive.exec.compress.output=true;
--写入时压缩生效
set hive.exec.orc.compression.strategy=COMPRESSION;
--分桶
set hive.enforce.bucketing=true; -- 开启分桶支持, 默认就是true
set hive.enforce.sorting=true; -- 开启强制排序

-- 优化: 
set hive.auto.convert.join=true;  -- map join
set hive.optimize.bucketmapjoin = true; -- 开启 bucket map join
-- 开启SMB map join
set hive.auto.convert.sortmerge.join=true;
set hive.auto.convert.sortmerge.join.noconditionaltask=true;
-- 写入数据强制排序
set hive.enforce.sorting=true;
set hive.optimize.bucketmapjoin.sortedmerge = true; -- 开启自动尝试SMB连接
```



```sql
-- 关闭hive自主优化
set hive.auto.convert.join=false;
-- 分区
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=10000;
set hive.exec.max.dynamic.partitions=100000;
set hive.exec.max.created.files=150000;
-- hive压缩
set hive.exec.compress.intermediate=true;
set hive.exec.compress.output=true;
-- 写入时压缩生效
set hive.exec.orc.compression.strategy=COMPRESSION;
-- 分桶
--set hive.enforce.bucketing=true;
--set hive.enforce.sorting=true;
--set hive.optimize.bucketmapjoin = true;
--set hive.auto.convert.sortmerge.join=true;
--set hive.auto.convert.sortmerge.join.noconditionaltask=true;
-- 并行执行
set hive.exec.parallel=true;
set hive.exec.parallel.thread.number=8;
-- 小文件合并
set mapred.max.split.size=2147483648;
set mapred.min.split.size.per.node=1000000000;
set mapred.min.split.size.per.rack=1000000000;
-- 矢量化查询
set hive.vectorized.execution.enabled=true;
-- 关联优化器
set hive.optimize.correlation=true;
-- 读取零拷贝
set hive.exec.orc.zerocopy=true;
-- join数据倾斜
set hive.optimize.skewjoin=false;
-- set hive.skewjoin.key=100000;
set hive.optimize.skewjoin.compiletime=false;
set hive.optimize.union.remove=false;
-- group倾斜
set hive.groupby.skewindata=false;
```



# HIVE优化

## 1.1hive的相关的优化

- hive的并行优化

  ```properties
  1) 并行编译
  	说明:hive在同一时刻只能编译一个会话中SQL，如果有多个会话一起来执行，此时出现排队的情况，只有当这一个会话中SQL全部编译后，才能编译另一个会话的SQL，导致执行效率变慢。
  解决方案:
  		hive.driver.parallel.compilation 是否开启并行编译 设置为true   
  		hive.driver.parallel.compilation.global.limit 最大允许同时有多少个SQL一起编译 设置为0表示无限制
  		说明:
  		这两项可以建议直接在CM的hive配置窗口上进行永久配置 (通用配置)
  2) 并行执行: 
  	说明:在运行一个SQL的时候，这个SQL形成的执行计划中，可能会被拆分为多个阶段，当各个阶段之间没有依赖关系的时候，可以尝试让多个阶段同时运行，从而提升运行的效率，这就是并行执行
  配置方案:
  		set hive.exec.parallel=true;  是否开启并行执行
  		set hive.exec.parallel.thread.number=16;  最大允许并行执行的数量
  	
  关于并行优化, 必须要求服务器有资源, 如果没有资源, 及时满足并行的条件, 也不会执行
  ```

- hive的小文件合并

  ```properties
  HDFS角度: 
  	每一个小文件, 都会有一份元数据, 当小文件过多后, 会导致出现大量的元数据存储namenonde的内存中, 从而导致内存使用率增大, 一旦namenode内存存满了, 即使datanode依然有空间, 也是无法存储的
  
  MR角度: 
  	在运行MR的时候, 每一个文件至少是一个文件切片, 也就意味至少需要运行一个mapTask. 当小文件过多后, 就会导致产生更多的mapTask, 而每一个mapTask只处理极少的数据, 导致资源被大量占用, 运行的时间都没有申请资源时间长
  	
  从hive角度如何解决小文件过多的问题呢?  在执行SQL的时候, 输出的文件数量尽量变得少一些
  
  hive.merge.mapfiles : 是否开启map端小文件合并 (适用于MR只有map没有reduce, map输出结果就是最终结果)
  hive.merge.mapredfiles : 是否开启reduce端小文件合并操作
  hive.merge.size.per.task: 合并后输出文件的最大值 ,默认是128M
  hive.merge.smallfiles.avgsize: 判断输出各个文件平均大小, 当这个大小小于设置值, 认为出现了小文件问题,需要进行合并操作
  
  比如说: 设置合并文件后, 输出最大值128M, 设置平均值为 50M
  	假设一个MR输出一下几个文件: 
  		 1M,10M,5M,3M,150M,80M,2M  平均值:35.xxx
  		
  		发现输出的多个文件的平均值比设定的平均值要小, 说明出现小文件的问题, 需要进行合并, 此时会合并结果为:
  		128M,123M
  ```

- 矢量化查询

  ```properties
  说明: 在hive读取数据的时候, 只需要读取跟SQL相关的列的数据即可, 不使用列, 不进行读取, 从而减少读取数据, 提升效率
  提前条件: 表的文件存储格式必须为ORC
  
  如何开启: 
  	set hive.exec.orc.zerocopy=true;
  
  示例: A表有 a,b,c,d,e 五个字段
  	select a,b,b  from A where b=xxx and c between xx and xxx;
  	发现SQL中没有使用d和e两个字段, 如果开启读取零拷贝, 在读取数据的时候, 就不会将d和e这两个字段读取到内存中
  ```



## 1.2 数据倾斜的优化

思考: 什么是数据倾斜呢?

```properties
  在运行过程中，会产生多个reduce，每个reduce所拿到的数据不均匀，其中有某几个或某一个reduce所拿到的数据远远大于或小于其他reduce所处理的数据，此时认为出现了数据倾斜问题。
```

思考:数据倾斜会导致问题?

```properties
1）执行效率下降（整个执行时间，取决于最后一个reduce结束时间）
2）由于其中某个或某几个reduce长时间运行，资源长期被占用，一旦超时，yarn会强制回收资源，导致运行失败
3）导致节点出现宕机问题
....
```

思考: 在执行什么SQL的时候, 会出现多个reduce的情况呢?

```properties
1）多表join的时候
2）执行group by的时候
3）执行分桶操作（但跟数据倾斜没太大关系）
```

如何解决数据倾斜的问题呢?

### 1.2.1 group by 数据倾斜

解决方案:

```properties
方案一:  采用combiner的方式来解决 (在map端提前聚合)
	核心: 在每一个mapTask进行提前聚合操作, 将聚合之后结果, 发送给reduce中, 完成最终的聚合, 从而减少从map到reduce的数据量, 减轻数据倾斜压力
	配置: 
		set hive.map.aggr=true;  开启map端提前聚合操作(combiner)


方案二:  负载均衡解决方案  (大combiner)
	核心: 采用两个MR来解决, 第一个MR负责将数据均匀落在不同reduce上, 进行聚合统计操作, 形成一个局部的结果, 在运行第二个MR读取第一个MR的局部结果, 按照相同key发往同一个reduce的方案, 完成最终聚合统计操作
	
	配置: 
		set hive.groupby.skewindata=true;

	注意:
		一旦使用方案二, hive不支持多列上的采用多次distinct去重操作, 一旦使用, 就会报错
			错误内容: DISTINCT on different columns notsupported with skew in data.
		示例: 
			(1) SELECT count(DISTINCT uid) FROM log
			(2) SELECT ip, count(DISTINCT uid) FROM log GROUP BY ip
			(3) SELECT ip, count(DISTINCT uid, uname) FROMlog GROUP BY ip
			(4) SELECT ip, count(DISTINCT uid), count(DISTINCT uname) FROMlog GROUP BY ip
			其中: 1,2,3 是可以正常执行的, 4会报错
```

### 1.3.2 join的数据倾斜

```properties
解决方案一 : 
	通过采用 map join,bucket map join, SMB map join
	方案: 将reduce端join的操作, 移植到map端进行join即可, 直接将倾斜排除即可, 因为在map端基本不会有倾斜问题

但是: 不管是map join, 还是 bucket map join以及SMB map join在使用的时候 都必须满足相关的条件, 但是很多时候, 我们的环境无法满足这些条件, 那么也就意味无法使用这些解决方案

解决方案二:
	思路: 将那些容易产生倾斜的key的值, 从这个环境中, 排除掉, 这样自然就没有倾斜问题, 讲这些倾斜的数据单独找一个MR来处理即可
	
	处理方案:  
		编译期解决方案:
			配置:
				set hive.optimize.skewjoin.compiletime=true;
			建表:
				CREATE TABLE list_bucket_single (key STRING, value STRING)
                -- 倾斜的字段和需要拆分的key值
                SKEWED BY (key) ON (1,5,6)
                --  为倾斜值创建子目录单独存放
                [STORED AS DIRECTORIES];
			说明:
				当明确知道表中那些key的值有倾斜问题, 一般擦用编译期解决, 在建表的时候, 提前设置好对应值有倾斜即可, 这样在执行的时候, hive会直接将这些倾斜的key的值从这个MR排除掉, 单独找一个MR来处理即可
		
		运行期解决方案:
			配置: 
				set hive.optimize.skewjoin=true; 是否开启运行期倾斜解决join
				set hive.skewjoin.key=100000; 当key出现多少个的时候, 认为有倾斜
			
			说明: 
				在执行的过程中, hive会记录每一个key出现的次数, 当出现次数达到设置的阈值后, 认为这个key有倾斜的问题, 直接将这个key对应数据排除掉, 单独找一个MR来处理即可


建议:
	如果提前知道表中有那些key有倾斜, 直接使用编译期即可
	如果仅知道一部分, 对于其他key无法保证, 建议编译期和运行期同时开启
```

union all相关优化点:

```properties
配置项:
	set hive.optimize.union.remove=true;
作用:
	此项配置减少对Union all子查询中间结果的二次读写

说明:
	此项配置一般和join的数据倾斜组合使用
```

### 1.3 总结说明

```properties
常开项: 
set hive.exec.parallel=true;  是否开启并行执行
set hive.exec.parallel.thread.number=16;  最大允许并行执行的数量
set hive.vectorized.execution.enabled=true; 矢量化查询
set hive.exec.orc.zerocopy=true; 读取零拷贝
set hive.optimize.correlation=true; 关联优化器

针对性开启:
set hive.map.aggr=true; 开启 group by combiner数据倾斜方案
set hive.groupby.skewindata=true;开启groupby 负载均衡优化

set hive.optimize.skewjoin.compiletime=true; join的编译期优化

set hive.optimize.skewjoin=true; 是否开启运行期倾斜解决join
set hive.skewjoin.key=100000; 当key出现多少个的时候, 认为有倾斜

set hive.optimize.union.remove=true; union all优化
```



```sql
set hive.auto.convert.join=false;
```

1. **指令作用**：

   - 这条指令将`hive.auto.convert.join`参数设置为`false`，禁用Hive的自动JOIN转换优化功能。

2. **参数含义**：

   - `hive.auto.convert.join`是Hive的一个优化参数，默认值为`true`

   - 当为`true`时，Hive会自动尝试将普通JOIN转换为Map Join（当小表符合条件时）

   - 当为`false`时，Hive会强制使用普通JOIN（Common Join/Shuffle Join）

     

## 2 hive压缩的配置

```properties
--hive压缩
set hive.exec.compress.intermediate=true;
set hive.exec.compress.output=true;
--写入时压缩生效
set hive.exec.orc.compression.strategy=COMPRESSION;


map中间结果压缩配置:
	建议: 在hive的会话窗口配置
	hive.exec.compress.intermediate: 是否hive对中间结果压缩
	
	以下两个建议直接在cm上yarn的配置目录下直接配置
	mapreduce.map.output.compress : 是否开启map阶段的压缩
	mapreduce.map.output.compress.codec : 选择什么压缩方案
		推荐配置:
			org.apache.hadoop.io.compress.SnappyCodec

reduce最终结果压缩配置:
	建议: 在hive的会话窗口配置
	hive.exec.compress.output: 是否开启对hive最终结果压缩配置
	
	以下两个建议直接在cm上yarn的配置目录下直接配置
	mapreduce.output.fileoutputformat.compress: 是否开启reduce端压缩配置
	mapreduce.output.fileoutputformat.compress.codec: 选择什么压缩方案
		推荐配置:
			org.apache.hadoop.io.compress.SnappyCodec
	mapreduce.output.fileoutputformat.compress.type : 压缩方案
		推荐配置:
			BLOCK
			
说明:
	如果hive上没有开启压缩, 及时配置MR的压缩, 那么也不会生效
```

- 动态生成分区的线程数

```properties
配置: hive.load.dynamic.partitions.thread

说明:
	在执行动态分区的时候, 最多允许多少个线程来运行动态分区操作, 线程越多 , 执行效率越高, 但是占用资源越大
默认值: 15
推荐:
	先采用默认, 如果动态分区执行慢, 而且还有剩余资源, 可以尝试调大
```

- 监听输入文件的线程数量

  ```properties
  配置项: hive.exec.input.listing.max.threads
  
  说明:
  	在运行SQL的时候, 可以使用多少个线程读取HDFS上数据, 线程越多, 读取效率越高, 占用资源越大
  
  默认值: 15
  推荐:
  	先采用默认, 如果读取数据执行慢, 而且还有剩余资源, 可以尝试调大
  ```

- hiveserver2的内存大小配置

  ```properties
  配置项: HiveServer2 的 Java 堆栈大小（字节）
  说明: 如果这个配置比较少, 在执行SQL的时候, 就会出现以下的问题:
  	此错误, 说明hiveserver2已经宕机了, 此时需要条件hiveserver2的内存大小,调整后, 重启
  ```

  ![image-20250806232220292](D:\MD\typora-images\image-20250806232220292.png)

## 3  hive动态分区的配置

```properties
当查询的时候, 指定分区字段, 可以减少查询表数据扫描量, 从而提升效率

1) 开启hive对动态分区的支持:set hive.exec.dynamic.partition=true;
	
2) 开启hive的非严格模式:set hive.exec.dynamic.partition.mode=nonstrict;
```



## 4 MapReduce基础配置

```properties
mapreduce.map.memory.mb : 在运行MR的时候, 一个mapTask需要占用多大内存
mapreduce.map.java.opts : 在运行MR的时候, 一个mapTask对应jvm需要占用多大内容

mapreduce.reduce.memory.mb: 在运行MR的时候, 一个reduceTask需要占用多大内存
mapreduce.reduce.java.opts : 在运行MR的时候, 一个reduceTask对应jvm需要占用多大内容

注意:
	jvm的内存配置要略小于对应内存
	所有内存配置大小, 不要超过nodemanager的内存大小
	
此处推荐: 
	一般不做任何修改 默认即可 
```



## 5  分桶表优化

![image-20250809115503200](D:\MD\typora-images\image-20250809115503200.png)

当多表进行join，可能会导致数据倾斜，或reduce压力过大处理时间超时报错。

出现类型与解决方案：

- ​	小表和大表:

​	采用 map join的方案：

```properties
在进行join的时候, 将小表的数据放置到每一个读取大表的mapTask的内存中, 让mapTask每读取一次大表的数据都和内存中小表的数据进行join操作, 将join上的结果输出到reduce端即可, 从而实现在map端完成join的操作

如何开启map Join
	set hive.auto.convert.join=true;  -- 是否开启map Join
	set hive.auto.convert.join.noconditionaltask.size=512000000; -- 设置小表最大的阈值(设置block cache 缓存大小)
	
map Join  不限制任何表
```



中型表和大表:

* 中型表: 与小表相比 大约是小表3~10倍左右

  ```properties
  解决方案: 
  
  * 1) 能提前过滤就提前过滤掉(一旦提前过滤后, 会导致中型表的数据量会下降, 有可能达到小表阈值)
    2) 如果join的字段值有大量的null, 可以尝试添加随机数(保证各个reduce接收数据量差不多的, 减少数据倾斜问题)
    3) 基于分桶表的: bucket map join
    
    bucket map join的生效条件:
  1） set hive.optimize.bucketmapjoin = true;  --开启bucket map join 支持
  2） 一个表的bucket数是另一个表bucket数的整数倍
  3） bucket列 == join列
  4） 必须是应用在map join的场景中
  
  注意：如果表不是bucket的，则只是做普通join。
  ```

大表和大表:

```properties
解决方案:
* 1. 能提前过滤就提前过滤掉(减少join之间的数量, 提升reduce执行效率)
* 2. 如果join的字段值有大量的null, 可以尝试添加随机数(保证各个reduce接收数据量差不多的, 减少数据倾斜问题)
* 3. SMB Map join (sort merge bucket map join)

实现SMB map join的条件要求: 
1） 一个表的bucket数等于另一个表bucket数(分桶数量是一致)
2） bucket列 == join列 == sort 列
3） 必须是应用在bucket map join的场景中
4) 开启相关的参数:
	-- 开启SMB map join
	set hive.auto.convert.sortmerge.join=true;
	set hive.auto.convert.sortmerge.join.noconditionaltask=true;
	--写入数据强制排序
	set hive.enforce.sorting=true;
	set hive.optimize.bucketmapjoin.sortedmerge = true; -- 开启自动尝试SMB连接
```

